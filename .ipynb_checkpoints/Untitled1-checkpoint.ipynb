{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nflgame\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#Setting for displaying inline graphics with matplotlib\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nflgame\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Custom color palettes for graphs\n",
    "custompal = [\"#F44336\",\"#E91E63\",\"#9C27B0\",\"#673AB7\",\"#3F51B5\",\"#2196F3\",\"#03A9F4\",\"#00BCD4\",\"#009688\",\"#4CAF50\",\n",
    "             \"#8BC34A\",\"#CDDC39\",\"#FFEB3B\",\"#FFC107\",\"#FF9800\",\"#FF5722\",\"#795548\",\"#9E9E9E\",\"#607D8B\"]\n",
    "minicustom = custompal[::2]\n",
    "sns.set_palette(minicustom)\n",
    "#sns.palplot(custompal)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "teams = ['buffalo-bills', 'miami-dolphins', 'new-england-patriots', 'new-york-jets', 'baltimore-ravens',\n",
    "         'cincinnati-bengals', 'cleveland-browns', 'pittsburgh-steelers', 'houston-texans', 'indianapolis-colts',\n",
    "         'jacksonville-jaguars', 'tennessee-titans', 'denver-broncos', 'kansas-city-chiefs', 'oakland-raiders',\n",
    "         'san-diego-chargers', 'dallas-cowboys', 'new-york-giants', 'philadelphia-eagles', 'washington-redskins',\n",
    "         'chicago-bears', 'detroit-lions', 'green-bay-packers', 'minnesota-vikings', 'atlanta-falcons',\n",
    "         'carolina-panthers', 'new-orleans-saints', 'tampa-bay-buccaneers', 'arizona-cardinals',\n",
    "         'san-francisco-49ers', 'seattle-seahawks', 'st-louis-rams']\n",
    "\n",
    "def GetStdTeam(team):\n",
    "    #Convert team name into easily parsable string\n",
    "    tm = team.replace('-',' ')\n",
    "\n",
    "    #Rams no longer in St. Louis (so just use Rams for search)\n",
    "    if (\"rams\" in tm):\n",
    "        tm = \"rams\"\n",
    "\n",
    "    #Get standard team name\n",
    "    stdteam = nflgame.standard_team(str(tm))\n",
    "    return stdteam\n",
    "\n",
    "\n",
    "def GetPenaltyData(team, year):\n",
    "    print 'Getting data for ' + team + ' ' + year\n",
    "\n",
    "    #Set up url string\n",
    "    teamfile = 'http://www.nflpenalties.com/team/' + team + '?year=' + year + '&view=log'\n",
    "\n",
    "    #Set up BeautifulSoup parser\n",
    "    r = urllib.urlopen(teamfile).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    print type(soup)\n",
    "\n",
    "    #Get content\n",
    "    content = soup.find_all(\"div\", id=\"content\")\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def ParseContent(content, stdteam):\n",
    "    tb = content[0].find('tbody')\n",
    "    y = pd.DataFrame()\n",
    "    z = ['team','penalty','game','opponent','player','position','week','year','penaltyindex']\n",
    "    count = 0\n",
    "    missing = 0\n",
    "\n",
    "    for row in tb.findAll('tr'):\n",
    "        missing = str(row).count('></a>')\n",
    "\n",
    "        if (missing == 0):\n",
    "            for link in row.findAll('a'):\n",
    "\n",
    "                href = link['href']\n",
    "                linkinfo = href.split('/')\n",
    "                linkcat = str(linkinfo[1])\n",
    "\n",
    "                linkdetails = str(link.get_text())\n",
    "\n",
    "                if (linkcat == 'penalty'):\n",
    "                    t = [stdteam]\n",
    "\n",
    "                if ('week.php' not in href):\n",
    "                    t = np.hstack([t, linkdetails])\n",
    "                else:\n",
    "                    weekandyear = href.split('week=')[1]\n",
    "                    week = weekandyear.split('?')[0]\n",
    "                    year = weekandyear.split('year=')[1]\n",
    "\n",
    "                    t = np.hstack([t, week, year])\n",
    "\n",
    "                    penaltyindex = stdteam + '-' + str(year) + '-' + str(count)\n",
    "\n",
    "                    t = np.hstack([t, penaltyindex])\n",
    "                    z = np.vstack([z,t])\n",
    "                    count += 1\n",
    "        else:\n",
    "            missing += 1\n",
    "\n",
    "    totalpenalties = missing + count\n",
    "    z['opponent'] = z['opponent'].apply(lambda x: GetStdTeam(x))\n",
    "    return [z,totalpenalties]\n",
    "\n",
    "#This writes CSV of data for particular team and year\n",
    "def WritePenaltyData(penalties, stdteam, year, fname):\n",
    "    totalnumpenalties = penalties[1]\n",
    "    print stdteam + ',' + year + ',' + str(totalnumpenalties)\n",
    "    np.savetxt(fname, penalties[0], fmt = '%s', delimiter=',')\n",
    "\n",
    "#Gets data from all teams into dataframe for particular year\n",
    "def GetAllTeams(year):\n",
    "    data = pd.DataFrame()\n",
    "    for team in teams:\n",
    "        stdteam = GetStdTeam(team)\n",
    "        print stdteam\n",
    "        fname = '/Users/kelly89/NFLPenalties/' + stdteam + year + '.csv'\n",
    "\n",
    "        if (os.path.isfile(fname) == False):\n",
    "            content = GetPenaltyData(team, year)\n",
    "            penalties = ParseContent(content, stdteam)\n",
    "            WritePenaltyData(penalties, stdteam, year, fname)\n",
    "        else:\n",
    "            print 'Found data file for ' + stdteam + ' ' + year\n",
    "\n",
    "        temp = pd.read_csv(fname, sep = ',')\n",
    "        data = pd.concat([data,temp], ignore_index = True)\n",
    "    return data\n",
    "\n",
    "#Writes main CSV (penalty data fro all teams and all years from 2011-2015)\n",
    "def GetAllYears(fname):\n",
    "    alldata = pd.DataFrame()\n",
    "    for year in ['2011','2012','2013','2014','2015']:\n",
    "        tempyear = GetAllTeams(year)\n",
    "        alldata = pd.concat([alldata,tempyear], ignore_index = True)\n",
    "\n",
    "    alldata.columns = ['Team','Penalty','NDate','Opponent','Player','Position','Week','Year','PenaltyIndex']\n",
    "    alldata['Opponent'] = alldata['Opponent'].apply(lambda x: GetStdTeam(x))\n",
    "    d = alldata.dropna()\n",
    "    d.to_csv(fname, sep = ',', index = False)\n",
    "\n",
    "def GetRegSeasonRecord (team, year):\n",
    "    team = str(team)\n",
    "    year = int(year)\n",
    "\n",
    "    games = nflgame.games_gen(year, week = None, home = team, away = team, kind = 'REG')\n",
    "    for g in games:\n",
    "        print g\n",
    "        if (g.winner == team):\n",
    "            w += 1\n",
    "        elif (g.loser == team):\n",
    "            l += 1\n",
    "        else:\n",
    "            print \"ERROR: can not determine winner of this game\"\n",
    "\n",
    "        print 'W-L: ' + str(w) + '-' + str(l)\n",
    "\n",
    "    return [int(w),int(l)]\n",
    "\n",
    "def GetAllRefInfo(fname):\n",
    "    pfr = 'http://www.pro-football-reference.com/officials'\n",
    "    pfrUrl = urllib.urlopen(pfr).read()\n",
    "    pfrSoup = BeautifulSoup(pfrUrl, \"lxml\")\n",
    "    content = pfrSoup.find_all(\"div\", id=\"page_content\")\n",
    "\n",
    "    a = content[0].find('table')\n",
    "    b = pd.DataFrame()\n",
    "    c = ['webpage','name','games','positions','years']\n",
    "    count = 0\n",
    "    m = 0\n",
    "    e = []\n",
    "\n",
    "    for row in a.findAll('tr')[1:]:\n",
    "        m = str(row).count('></td>')\n",
    "        webpage = os.path.dirname(pfr) + str(row.findNext('a')['href'])\n",
    "        e = [webpage]\n",
    "        for val in row.findAll('td'):\n",
    "            d = val.get_text()\n",
    "            e = np.hstack([e, str(d)])\n",
    "        if (len(e) == 5):\n",
    "            c = np.vstack([c,e])\n",
    "            count += 1\n",
    "        else:\n",
    "            m += 1\n",
    "    GetAllRefGames(c[1:], fname)\n",
    "\n",
    "def GetAllRefGames(refinfo, fname):\n",
    "    refstats = ['ref','exp','date','visitor','home','position','vpen','vpenyd','hpen','hpenyd']\n",
    "\n",
    "    for ref in refinfo:\n",
    "        refname = ref[1].rpartition(' ')[2]\n",
    "        exp = str(ref[2])\n",
    "        print 'Getting data for ' + refname\n",
    "\n",
    "        refUrl = urllib.urlopen(ref[0]).read()\n",
    "        refSoup = BeautifulSoup(refUrl, \"lxml\")\n",
    "\n",
    "        i = refSoup.find_all(\"div\", id=\"div_game_logs\")[0]\n",
    "\n",
    "        for row in i.findAll('tr'):\n",
    "            j = []\n",
    "            alltd = row.findAll(\"td\")\n",
    "            if (len(alltd) != 0):\n",
    "                date = alltd[0].get_text()\n",
    "                teams = alltd[1].get_text().split(' @ ')\n",
    "                v = nflgame.standard_team(teams[0].rpartition(' ')[2])\n",
    "                h = nflgame.standard_team(teams[1].rpartition(' ')[2])\n",
    "                pos = alltd[2].get_text().replace(' ','-')\n",
    "                vpen = alltd[4].get_text()\n",
    "                vpenyd = alltd[5].get_text()\n",
    "                hpen = alltd[7].get_text()\n",
    "                hpenyd = alltd[8].get_text()\n",
    "\n",
    "                j = np.hstack([refname, exp, date, v, h, pos, vpen, vpenyd, hpen, hpenyd])\n",
    "\n",
    "                if (len(j) == 10):\n",
    "                    refstats = np.vstack([refstats, j])\n",
    "            print refstats\n",
    "    np.savetxt(fname, refstats, fmt = '%s', delimiter=',')\n",
    "\n",
    "\n",
    "def NormDate(date):\n",
    "    d = date.split('-')\n",
    "    d = str(d[1] + '/' + d[2] + '/' + d[0])\n",
    "    return str(d)\n",
    "\n",
    "def MakeKey(r):\n",
    "    yr = r.NDate.replace('/','-')\n",
    "    k = str(yr + '-' + r.HTeam)\n",
    "    return k\n",
    "\n",
    "def GetHomeTm(r, homeTms):\n",
    "    if (r.Team in homeTms):\n",
    "        return r.Team\n",
    "    else:\n",
    "        return r.Opponent\n",
    "\n",
    "\n",
    "def GetAwayTm(r, homeTms):\n",
    "    if (r.Team not in homeTms):\n",
    "        return r.Team\n",
    "    else:\n",
    "        return r.Opponent\n",
    "\n",
    "def MergePenaltyAndRefs(fname, data, rd):\n",
    "\n",
    "    rd['NDate'] = rd['Date'].apply(lambda x: NormDate(x))\n",
    "    #rd['Year'] = rd['NDate'].apply(lambda x: str(x).split('/')[2])\n",
    "\n",
    "    #penYears = map(str, range(2011,2016))\n",
    "    #rd = rd[rd['Year'].isin(penYears)]\n",
    "\n",
    "    #Add column to determine if player was on home team\n",
    "    a = rd.groupby(['HTeam', 'NDate'], as_index = False).count()\n",
    "    data['HTeam'] = data.apply(lambda x: GetHomeTm(x, a[(a['NDate'] == str(x.NDate))]), axis=1)\n",
    "    data['VTeam'] = data.apply(lambda x: GetAwayTm(x, a[(a['NDate'] == str(x.NDate))]), axis=1)\n",
    "\n",
    "    #This isn't necessary and sort of hackish. Probably better to multindex?\n",
    "    rd['key'] = rd.apply(MakeKey, axis=1)\n",
    "    data['key'] = data.apply(MakeKey, axis=1)\n",
    "\n",
    "    uniqcols = rd.columns.difference(data.columns)\n",
    "    newrd = pd.concat([rd[['key']],rd[uniqcols]], axis=1)\n",
    "    #data = data.set_index('key')\n",
    "    #rd = rd.set_index('key')\n",
    "    #newrd = newrd.set_index('key')\n",
    "\n",
    "\n",
    "    c = pd.merge(data, newrd, left_on = 'key', right_on = 'key')\n",
    "    #d = c.update(rd)\n",
    "    print c.shape\n",
    "    print c.head(40)\n",
    "    c.to_csv(fname, sep=\",\", index = False)\n",
    "\n",
    "\n",
    "def GetGameData(year):\n",
    "    games = nflgame.games_gen(year)\n",
    "    a = pd.DataFrame()\n",
    "    for g in games:\n",
    "        Schedule = g.schedule\n",
    "        s = pd.DataFrame.from_dict([Schedule])\n",
    "        extra = {'HScore': g.score_home, 'VScore': g.score_away, 'Winner': g.winner, 'Season': g.season()}\n",
    "        e = pd.DataFrame.from_dict([extra])\n",
    "        r = pd.concat([s, e], axis=1)\n",
    "        r['HVScoreDiff'] = r['HScore'] - r['VScore']\n",
    "        r['NDate'] = str(str(r.month[0]) + '/' + str(r.day[0]) + '/' + str(r.year[0]))\n",
    "        a = pd.concat([a, r])\n",
    "    return a\n",
    "\n",
    "def GetAllGameData(fname):\n",
    "    allyears = pd.DataFrame()\n",
    "    hdr = ['HScore','HVScoreDiff','NDate','Season','VScore','Winner','VTeam','day','eid','gamekey','HTeam','meridiem','month','gametype','time','wday','week','year']\n",
    "    for year in range(2009,2016):\n",
    "        yr = GetGameData(year)\n",
    "        print 'Getting game data for ' + str(year) + ' season'\n",
    "        allyears = pd.concat([allyears, yr])\n",
    "    allyears.columns = [hdr]\n",
    "    allyears = allyears.drop(['meridiem','year'], axis = 1)\n",
    "    allyears.to_csv(fname, sep=\",\", index = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Main code\n",
    "\n",
    "#Get penalty data for all years into dataframe\n",
    "allPenF = '/Users/kelly89/NFLPenalties/AllPenalties.csv'\n",
    "if (os.path.isfile(allPenF) == False):\n",
    "    GetAllYears(allPenF)\n",
    "else:\n",
    "    print 'Found ' + allPenF\n",
    "data = pd.read_csv(allPenF, sep=\",\")\n",
    "print data.shape\n",
    "\n",
    "#Get all game data\n",
    "gameDataF = '/Users/kelly89/NFLPenalties/AllGameData.csv'\n",
    "if (os.path.isfile(gameDataF) == False):\n",
    "    GetAllGameData(gameDataF)\n",
    "else:\n",
    "    print 'Found ' + gameDataF\n",
    "gd = pd.read_csv(gameDataF, sep=',')\n",
    "print gd.shape\n",
    "\n",
    "uniqcols = data.columns.difference(gd.columns)\n",
    "print data[uniqcols].columns.values\n",
    "\n",
    "gd['Team'] = gd['HTeam']\n",
    "gpd = pd.merge(data[uniqcols], gd, on = 'Team')\n",
    "\n",
    "print gpd.columns.values\n",
    "gpd = gpd.drop('Team', axis = 1)\n",
    "\n",
    "uniq = data.columns.difference(gpd.columns)\n",
    "print data[uniq].columns.values\n",
    "\n",
    "gpd['Team'] = gpd['VTeam']\n",
    "m = pd.merge(gpd[uniq], data, on = 'Team')\n",
    "\n",
    "print m.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a['IsHomeTeam'] = np.where((a['Team'] == a['HTeam']), True, False)\n",
    "a['NumPenalty'] = np.where((a['Team'] == a['HTeam']), a['HPen'], a['VPen'])\n",
    "a['PenaltyYds'] = np.where((a['Team'] == a['HTeam']), a['HPenYd'], a['VPenYd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uniqcols = gd.columns.difference(data.columns)\n",
    "newgd = pd.concat([gd['NDate-Team'],gd[uniqcols]], axis=1)\n",
    "    \n",
    "print newgd.head(5)\n",
    "print gd.head(5)\n",
    "print data.head(5)\n",
    "\n",
    "#gpd = pd.merge(data, newgd, left_on = 'NDate-Team', right_on = 'NDate-Team')\n",
    "#print gpd.shape\n",
    "#print gpd.head(15)\n",
    "\n",
    "#Get refdata into dataframe\n",
    "refDataF = '/Users/kelly89/NFLPenalties/RefData.csv'\n",
    "if (os.path.isfile(refDataF) == False):\n",
    "    GetAllRefInfo(refDataF)\n",
    "else:\n",
    "    print 'Found ' + refDataF\n",
    "rd = pd.read_csv(refDataF, sep=\",\")\n",
    "rd.columns = ['LastName','Exp','Date','VTeam','HTeam','RefPosition','VPen','VPenYd','HPen','HPenYd']\n",
    "rd = rd[(rd['RefPosition'] == 'Referee')]\n",
    "print rd.shape\n",
    "\n",
    "#Merge ref data with penalty data\n",
    "refPenF = '/Users/kelly89/NFLPenalties/RefAndPenalties.csv'\n",
    "if (os.path.isfile(refPenF) == False):\n",
    "    MergePenaltyAndRefs(refPenF, data, rd)\n",
    "else:\n",
    "    print 'Found ' + refPenF\n",
    "rp = pd.read_csv(refPenF, sep=',')\n",
    "print rp.shape\n",
    "print 'rp descriptive stats:'\n",
    "for c in rp.columns.values:\n",
    "    n = rp[c].isnull().sum()\n",
    "    print c + ' ' + str(n)\n",
    "\n",
    "#Get all game data\n",
    "gameDataF = '/Users/kelly89/NFLPenalties/AllGameData.csv'\n",
    "if (os.path.isfile(gameDataF) == False):\n",
    "    GetAllGameData(gameDataF)\n",
    "else:\n",
    "    print 'Found ' + gameDataF\n",
    "gd = pd.read_csv(gameDataF, sep=',')\n",
    "print gd.shape\n",
    "print 'gd descriptive stats:'\n",
    "for c in gd.columns.values:\n",
    "    n = gd[c].isnull().sum()\n",
    "    print c + ' ' + str(n)\n",
    "\n",
    "#Merge ref, penalty, and gamedata\n",
    "rfpF = '/Users/kelly89/NFLPenalties/RefPenaltyAndGameData.csv'\n",
    "if (os.path.isfile(rfpF) == False):\n",
    "    gd['key'] = gd.apply(lambda x: MakeKey(x), axis=1)\n",
    "    print 'gd'\n",
    "    print gd.head(10)\n",
    "    uniqcols = rd[rp.columns.difference(data.columns),'key']\n",
    "    #newrp = pd.concat([rp[['key']],rp[uniqcols]], axis=1)\n",
    "    print 'uniqcols'\n",
    "    print uniqcols.head(10)\n",
    "    #m = pd.merge(gd, newrp, left_on = 'key', right_on = 'key', how = 'outer')\n",
    "    #print m.shape\n",
    "    #m.to_csv(rfpF, sep = ',', index = False)\n",
    "else:\n",
    "    print 'Found ' + rfpF\n",
    "#rfp = pd.read_csv(rfpF, sep=',')\n",
    "#print rfp.shape\n",
    "#print 'rfp descriptive stats:'\n",
    "\n",
    "#for c in rfp.columns.values:\n",
    "#    n = rfp[c].isnull().sum()\n",
    "#    print c + ' ' + str(n)\n",
    "\n",
    "#sdF = '/Users/kelly89/NFLPenalties/StadiumData.csv'\n",
    "#sd = pd.read_csv(sdF, sep = ',')\n",
    "#print sd.shape\n",
    "\n",
    "#print sd.columns.values\n",
    "#print rfp.columns.values \n",
    "\n",
    "#rfpgsF = '/Users/kelly89/NFLPenalties/RefPenaltyGameAndStadiumData.csv'\n",
    "#if (os.path.isfile(rfpgsF) == False):\n",
    "#    m = pd.merge(rfp, sd, left_on = 'HTeam', right_on = 'HTeam')\n",
    "#    m.to_csv(rfpgsF, sep = ',', index = False)\n",
    "#else:\n",
    "#    print 'Found ' + rfpgsF\n",
    "#f = pd.read_csv(rfpgsF, sep = ',')\n",
    "#print f.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
